{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import sys\n",
    "import time\n",
    "from lexsub_xml import read_lexsub_xml\n",
    "\n",
    "# suggested imports \n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "import string\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "# Participate in the 4705 lexical substitution competition (optional): NO\n",
    "# Alias: [please invent some name]\n",
    "\n",
    "def tokenize(s):\n",
    "    s = \"\".join(\" \" if x in string.punctuation else x for x in s.lower())    \n",
    "    return s.split() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidates(lemma, pos):\n",
    "    # Part 1\n",
    "    possible_synonyms = []\n",
    "    l1 = wn.lemmas(lemma, pos=pos)\n",
    "    for lexeme in l1:\n",
    "        syn = lexeme.synset().lemmas()\n",
    "        for i in range(len(syn)):\n",
    "            if syn[i].name() in possible_synonyms or syn[i].name() == lemma:\n",
    "                continue\n",
    "            elif \"_\" in syn[i].name():\n",
    "                possible_synonyms.append(\" \".join(syn[i].name().split(\"_\")))\n",
    "            else:\n",
    "                possible_synonyms.append(syn[i].name())          \n",
    "    return possible_synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smurf_predictor(context):\n",
    "    \"\"\"\n",
    "    Just suggest 'smurf' as a substitute for all words.\n",
    "    \"\"\"\n",
    "    return 'smurf'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wn_frequency_predictor(context):\n",
    "    l = wn.lemmas(context.lemma, pos = context.pos)\n",
    "    pairs = defaultdict(int)\n",
    "    for lexeme in l:\n",
    "        syn = lexeme.synset().lemmas()\n",
    "        for i in syn:\n",
    "            if i.name() == context.lemma:\n",
    "                continue\n",
    "            pairs[i.name()] += i.count()\n",
    "    return max(pairs.items(), key = lambda a: a[1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wn_simple_lesk_predictor(context):\n",
    "    l = wn.lemmas(context.lemma, pos = context.pos)\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words = set(stop_words)\n",
    "    max_length = 0\n",
    "    best = None\n",
    "    for lexeme in l:\n",
    "        syn = lexeme.synset()\n",
    "        defin = set(tokenize(syn.definition()))\n",
    "        for i in syn.examples():\n",
    "            defin = defin.union(set(tokenize(i)))\n",
    "        for j in syn.hypernyms():\n",
    "            defin = defin.union(set(tokenize(j.definition())))\n",
    "            for k in j.examples():\n",
    "                defin = defin.union(set(tokenize(k)))\n",
    "        defin -= stop_words\n",
    "        contextset = set(tokenize(str(context)))\n",
    "        contextset -= stop_words\n",
    "        intersect = defin.intersection(contextset)\n",
    "        if len(intersect) > max_length:\n",
    "            best = syn\n",
    "            max_length = len(intersect)\n",
    "    if best is None:\n",
    "        return wn_frequency_predictor(context)\n",
    "    poss = {}\n",
    "    for word in best.lemmas():\n",
    "        if word.name() == context.lemma:\n",
    "            continue\n",
    "        else:\n",
    "            poss[word.name()] = word.count()\n",
    "    if not bool(poss) or set(poss.values()) == set([0]):\n",
    "        return wn_frequency_predictor(context)\n",
    "    return max(poss.items(), key = lambda a: a[1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos(v1,v2):\n",
    "        return np.dot(v1,v2) / (np.linalg.norm(v1)*np.linalg.norm(v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecSubst(object):\n",
    "        \n",
    "    def __init__(self, filename):\n",
    "        self.model = gensim.models.KeyedVectors.load_word2vec_format(filename, binary=True)    \n",
    "\n",
    "    def predict_nearest(self,context):\n",
    "        pos_syn = get_candidates(context.lemma, context.pos)\n",
    "        sim = 0\n",
    "        guess = None\n",
    "        guesses = {}\n",
    "        for syn in pos_syn:\n",
    "            try:\n",
    "                guesses[syn] = self.model.similarity(syn, context.lemma)\n",
    "                if self.model.similarity(syn, context.lemma) > sim:\n",
    "                    guess = syn\n",
    "                    sim = self.model.similarity(syn, context.lemma)\n",
    "            except KeyError:\n",
    "                continue\n",
    "        print(\"NO CONTEXT\\n\",guesses)\n",
    "        return guess\n",
    "    \n",
    "\n",
    "    def predict_nearest_with_context(self, context): \n",
    "        vec = np.array(self.model.wv[context.lemma])\n",
    "        count = 0\n",
    "        revLcont = context.left_context\n",
    "        revLcont.reverse()\n",
    "        Rcont = context.right_context\n",
    "        stop_words = stopwords.words('english')\n",
    "        stop_words = set(stop_words)\n",
    "        pos_syn = get_candidates(context.lemma, context.pos)\n",
    "        for i in revLcont:\n",
    "            if i in stop_words or i in string.punctuation:\n",
    "                continue\n",
    "            elif count > 4:\n",
    "                break\n",
    "            else:\n",
    "                try:\n",
    "                    vec += np.array(self.model.wv[i])\n",
    "                    count += 1\n",
    "                except KeyError:\n",
    "                    continue\n",
    "        count = 0\n",
    "        for j in Rcont:\n",
    "            if j in stop_words or j in string.punctuation:\n",
    "                continue\n",
    "            elif count > 4:\n",
    "                break\n",
    "            else:\n",
    "                try:\n",
    "                    vec += np.array(self.model.wv[j])\n",
    "                    count += 1\n",
    "                except KeyError:\n",
    "                    continue\n",
    "        sim = 0\n",
    "        for syn in pos_syn:\n",
    "            try:\n",
    "                if cos(self.model.wv[syn], vec) > sim:\n",
    "                    guess = syn\n",
    "                    sim = cos(self.model.wv[syn], vec)\n",
    "            except KeyError:\n",
    "                continue\n",
    "        return guess\n",
    "    \n",
    "    def new_predict(self, context): # Add a snapier name after completion\n",
    "        vec = np.array(self.model.wv[context.lemma])\n",
    "        count = 0\n",
    "        revLcont = context.left_context\n",
    "        revLcont.reverse()\n",
    "        Rcont = context.right_context\n",
    "        stop_words = stopwords.words('english')\n",
    "        stop_words = set(stop_words)\n",
    "        pos_syn = get_candidates(context.lemma, context.pos)\n",
    "        for i in revLcont:\n",
    "            if i in stop_words or i in string.punctuation:\n",
    "                continue\n",
    "            elif count > 4:\n",
    "                break\n",
    "            else:\n",
    "                try:\n",
    "                    vec += np.array(self.model.wv[i])\n",
    "                    count += 1\n",
    "                except KeyError:\n",
    "                    continue\n",
    "        count = 0\n",
    "        for j in Rcont:\n",
    "            if j in stop_words or j in string.punctuation:\n",
    "                continue\n",
    "            elif count > 4:\n",
    "                break\n",
    "            else:\n",
    "                try:\n",
    "                    vec += np.array(self.model.wv[j])\n",
    "                    count += 1\n",
    "                except KeyError:\n",
    "                    continue\n",
    "        guesses = {}\n",
    "        for syn in pos_syn:\n",
    "            try:\n",
    "                guesses[syn] = cos(self.model.wv[syn], vec)\n",
    "            except KeyError:\n",
    "                continue\n",
    "        print(\"CONTEXT\\n\",guesses)        return max(guesses.items(), key = lambda a: a[1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2VMODEL_FILENAME = 'GoogleNews-vectors-negative300.bin.gz'\n",
    "predictor = Word2VecSubst(W2VMODEL_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "filename = \"freq.predict\"\n",
    "file = open(filename, \"w\")\n",
    "count = 0\n",
    "for context in read_lexsub_xml(sys.argv[1]):\n",
    "    prediction1 = predictor.new_predict(context)\n",
    "    prediction2 = predictor.predict_nearest(context)\n",
    "    #print(prediction)\n",
    "    print(context)  # useful for debugging\n",
    "    if count == 2:\n",
    "        break\n",
    "    count += 1\n",
    "    file.write(\"{}.{} {} :: {}\".format(context.lemma, context.pos, context.cid, prediction))\n",
    "    file.write(\"\\n\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
